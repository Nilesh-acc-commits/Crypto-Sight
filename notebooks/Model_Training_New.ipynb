{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Training Pipeline - 15-Step Workflow\n",
                "## Coins: ADA, BNB, BTC, DOGE, ETH\n",
                "\n",
                "This notebook implements the strict 15-step workflow requested:\n",
                "1.  **Load Data** (Local CSVs)\n",
                "2.  **EDA** (Info, Describe)\n",
                "3.  **Null Checks**\n",
                "4.  **IQR Outlier Cleaning**\n",
                "5.  **Datetime Conversion**\n",
                "6.  **Feature Extraction**\n",
                "7.  **Drop NaNs**\n",
                "8.  **Split X/y**\n",
                "9.  **Train/Test Split**\n",
                "10. **Normalization**\n",
                "11. **Cross-Validation & Tuning**\n",
                "12. **Train Models** (LR, RF, XGB, LSTM)\n",
                "13. **Final Score**\n",
                "14. **Save Models**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "import warnings\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from xgboost import XGBRegressor\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
                "\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Configuration\n",
                "COINS = ['ADA', 'BNB', 'BTC', 'DOGE', 'ETH']\n",
                "DATA_DIR = 'data'\n",
                "MODELS_DIR = 'models'\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper Functions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Feature Extraction\nCalculate technical indicators (SMA, RSI, Volatility, Lags) and the target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_outliers_iqr(df, columns):\n",
                "    \"\"\"Step 4: Remove outliers using IQR method.\"\"\"\n",
                "    df_clean = df.copy()\n",
                "    for col in columns:\n",
                "        Q1 = df_clean[col].quantile(0.25)\n",
                "        Q3 = df_clean[col].quantile(0.75)\n",
                "        IQR = Q3 - Q1\n",
                "        lower_bound = Q1 - 1.5 * IQR\n",
                "        upper_bound = Q3 + 1.5 * IQR\n",
                "        # Filter\n",
                "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
                "    return df_clean\n",
                "\n",
                "def process_features(df):\n",
                "    \"\"\"Step 6: Feature Extraction.\"\"\"\n",
                "    df = df.copy()\n",
                "    # Ensure close is float\n",
                "    df['close'] = df['close'].astype(float)\n",
                "    \n",
                "    # SMA & RSI\n",
                "    df['SMA_20'] = df['close'].rolling(20).mean()\n",
                "    delta = df['close'].diff()\n",
                "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
                "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
                "    rs = gain / loss\n",
                "    df['RSI_14'] = 100 - (100 / (1 + rs))\n",
                "    \n",
                "    # Lags\n",
                "    for lag in [1, 2, 3, 7]:\n",
                "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
                "        \n",
                "    # Volatility\n",
                "    df['volatility_20'] = df['close'].rolling(20).std()\n",
                "    \n",
                "    # Target (Next Day Close)\n",
                "    df['target'] = df['close'].shift(-1)\n",
                "    \n",
                "    return df\n",
                "\n",
                "def evaluate_model(y_true, y_pred, model_name, horizon):\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    return {'Model': model_name, 'Horizon': horizon, 'R2': r2, 'RMSE': rmse, 'MAE': mae}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Main Processing Loop"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 0: Initialization\nInitialize the storage dictionary to hold data for all coins."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline_data = {}\n",
                "print(\"Initialized pipeline_data storage.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Load Data\nLoad the pre-processed CSV files for each coin."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 1: Load Data\\n{'='*50}\")\n",
                "for coin in COINS:\n",
                "    print(f\"Processing {coin}...\")\n",
                "    file_path = f\"{DATA_DIR}/{coin}_ML_ready.csv\"\n",
                "    if not os.path.exists(file_path):\n",
                "        print(f\"File not found: {file_path}\")\n",
                "        continue\n",
                "    \n",
                "    df = pd.read_csv(file_path)\n",
                "    print(f\"Loaded {len(df)} rows.\")\n",
                "    pipeline_data[coin] = {'df': df}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Exploratory Data Analysis (EDA)\nDisplay basic information and statistics for the loaded data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 2: EDA\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    print(f\"\\n--- {coin} EDA ---\")\n",
                "    df = data['df']\n",
                "    print(df.info())\n",
                "    print(df.describe().T[['mean', 'std', 'min', 'max']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Null Value Checks\nCheck for any missing values in the datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 3: Null Checks\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    print(f\"\\n--- {coin} Null Checks ---\")\n",
                "    df = data['df']\n",
                "    print(df.isnull().sum()[df.isnull().sum() > 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Datetime Conversion\nConvert the `open_time` column to datetime objects and sort by time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 5: Datetime Conversion\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    df = data['df']\n",
                "    if 'open_time' in df.columns:\n",
                "        df['open_time'] = pd.to_datetime(df['open_time'])\n",
                "        df = df.sort_values('open_time')\n",
                "    pipeline_data[coin]['df'] = df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Outlier Cleaning (IQR)\nRemove outliers from `close` and `volume` columns using the Interquartile Range method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 4: IQR Cleaning\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    print(f\"\\n--- {coin} IQR Cleaning ---\")\n",
                "    df = data['df']\n",
                "    initial_len = len(df)\n",
                "    df = remove_outliers_iqr(df, ['close', 'volume'])\n",
                "    print(f\"Removed {initial_len - len(df)} outliers.\")\n",
                "    pipeline_data[coin]['df'] = df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Feature Extraction\nCalculate technical indicators (SMA, RSI, Volatility, Lags) and the target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 6: Feature Extraction\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    df = data['df']\n",
                "    df = process_features(df)\n",
                "    pipeline_data[coin]['df'] = df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 7: Drop NaN Values\nRemove rows with missing values created during feature extraction (e.g., rolling windows)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 7: Drop NaNs\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    df = data['df']\n",
                "    df = df.dropna()\n",
                "    print(f\"{coin}: Rows after dropping NaNs: {len(df)}\")\n",
                "    pipeline_data[coin]['df'] = df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 8: Split Features (X) and Target (y)\nSeparate the feature columns from the target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 8: Split X/y\\n{'='*50}\")\n\n",
                "feature_cols = ['SMA_20', 'RSI_14', 'volatility_20', 'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_7']\n\n",
                "for coin, data in pipeline_data.items():\n\n",
                "    df = data['df']\n\n",
                "    pipeline_data[coin]['X'] = df[feature_cols]\n\n",
                "    pipeline_data[coin]['y'] = df['target']\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 9: Train/Test Split\nSplit the data into training and testing sets using a time-series split (80/20)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 9: Train/Test Split\\n{'='*50}\")\n\n",
                "for coin, data in pipeline_data.items():\n\n",
                "    X = data['X']\n\n",
                "    y = data['y']\n\n",
                "    split_idx = int(len(X) * 0.8)\n\n",
                "    pipeline_data[coin]['X_train_raw'] = X.iloc[:split_idx]\n\n",
                "    pipeline_data[coin]['X_test_raw'] = X.iloc[split_idx:]\n\n",
                "    pipeline_data[coin]['y_train'] = y.iloc[:split_idx]\n\n",
                "    pipeline_data[coin]['y_test'] = y.iloc[split_idx:]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 10: Normalization (MinMaxScaler)\nScale the features to a range of [0, 1] using MinMaxScaler."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 10: Normalization\\n{'='*50}\")\n",
                "for coin, data in pipeline_data.items():\n",
                "    scaler = MinMaxScaler()\n",
                "    X_train = scaler.fit_transform(data['X_train_raw'])\n",
                "    X_test = scaler.transform(data['X_test_raw'])\n",
                "    \n",
                "    pipeline_data[coin]['X_train'] = X_train\n",
                "    pipeline_data[coin]['X_test'] = X_test\n",
                "    \n",
                "    os.makedirs(f'{MODELS_DIR}/{coin}', exist_ok=True)\n",
                "    joblib.dump(scaler, f'{MODELS_DIR}/{coin}/scaler.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 11 & 12: Cross-Validation & Model Training\nTrain a Linear Regression model for each coin. (Cross-validation setup is included but LR doesn't require tuning)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 11 & 12: Tuning & Training\\n{'='*50}\")\n\n",
                "tscv = TimeSeriesSplit(n_splits=5)\n\n",
                "for coin, data in pipeline_data.items():\n\n",
                "    print(f\"Training Linear Regression for {coin}...\")\n\n",
                "    lr = LinearRegression()\n\n",
                "    lr.fit(data['X_train'], data['y_train'])\n\n",
                "    pipeline_data[coin]['model'] = lr\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 13: Final Evaluation & Scoring\nEvaluate the trained models on the test set and print the performance metrics (R2, MAE, MSE)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*50}\\nStep 13: Final Score & Evaluation\\n{'='*50}\")\n\n",
                "print(f\"\\n{'Coin':<10} | {'Model':<20} | {'Train R2':<10} | {'Test R2':<10} | {'MAE':<10} | {'MSE':<10}\")\n\n",
                "print(\"-\" * 90)\n\n",
                "\n\n",
                "for coin, data in pipeline_data.items():\n\n",
                "    model = data['model']\n\n",
                "    X_train = data['X_train']\n\n",
                "    X_test = data['X_test']\n\n",
                "    y_train = data['y_train']\n\n",
                "    y_test = data['y_test']\n\n",
                "    \n\n",
                "    train_pred = model.predict(X_train)\n\n",
                "    test_pred = model.predict(X_test)\n\n",
                "    \n\n",
                "    joblib.dump(model, f'{MODELS_DIR}/{coin}/LinearRegression_model.pkl')\n\n",
                "    \n\n",
                "    train_r2 = r2_score(y_train, train_pred)\n\n",
                "    test_r2 = r2_score(y_test, test_pred)\n\n",
                "    mae = mean_absolute_error(y_test, test_pred)\n\n",
                "    mse = mean_squared_error(y_test, test_pred)\n\n",
                "    \n\n",
                "    print(f\"{coin:<10} | {'LinearRegression':<20} | {train_r2:<10.4f} | {test_r2:<10.4f} | {mae:<10.4f} | {mse:<10.4f}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}